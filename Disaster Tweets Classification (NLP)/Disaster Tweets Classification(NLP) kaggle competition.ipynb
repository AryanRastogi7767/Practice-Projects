{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columns**\n",
    "\n",
    "1. id - a unique identifier for each tweet\n",
    "2. text - the text of the tweet\n",
    "3. location - the location the tweet was sent from (may be blank)\n",
    "4. keyword - a particular keyword from the tweet (may be blank)\n",
    "5. target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "\n",
    "**Evaluation metric: F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "(7613, 5)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print(train.head(3))\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "print(test.head(3))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  target\n",
      "0   0       0\n",
      "1   2       0\n",
      "2   3       0\n",
      "(3263, 2)\n"
     ]
    }
   ],
   "source": [
    "sample= pd.read_csv('sample_submission.csv')\n",
    "print(sample.head(3))\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts() #data is approximately balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24b3e24cf60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the null values.\n",
    "fig = plt.figure(figsize = (8,5))\n",
    "sns.heatmap(train.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    7552\n",
       "True       61\n",
       "Name: keyword, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['keyword'].isnull().value_counts() # 61 values out of 7613 are missing in the 'keyword' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5080\n",
       "True     2533\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['location'].isnull().value_counts() # 2533 values out of 7613 are missing in the 'location' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['location'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "       'cliff%20fall', 'collapse', 'collapsed', 'collide'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['keyword'].unique()[:50] # there are 221 unique values including nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this sample we can see that there are many values in this column which mean the same and therefore should be replaced by a common word (Stemming). Some values have '%20' in them which will be replaced by \" \".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = train[train['target'] == 1] # creating separate dataframes for both the classes to do class specific analysis.\n",
    "fake = train[train['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1513"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real['location'].nunique() # we can see that only 1513 out of 3341 unique locations are in real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that only 1513 out of 3341 unique locations are in real data. So, intuitively, we can build a simple classifier (for the given data) using if else saying that:**\n",
    "\n",
    "**If the location of the query point is in the 1513 unique values then the tweet is real else it is fake.**\n",
    "\n",
    "**But this is not really a proper logical solution to our problem as there will be some locations that will be common in both the datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2142"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake['location'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this we can see that there are 314 common locations in both the datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['keyword'].isnull()==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    42\n",
       "0    19\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['keyword'].isnull()==True]['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1458\n",
       "1    1075\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['location'].isnull()==True]['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ablaze</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword\n",
       "31  ablaze\n",
       "32  ablaze\n",
       "33  ablaze\n",
       "34  ablaze\n",
       "35  ablaze"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a new dataframe that consists on non null values of the keywords\n",
    "keywords = pd.DataFrame(train[train['keyword'].isnull()== False]['keyword'])\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 7613/7613 [00:00<00:00, 691961.09it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_key = []\n",
    "for word in tqdm(train['keyword'].values):\n",
    "    key = str(word).replace(\"%20\",\" \")\n",
    "    processed_key.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As keywords are usually the main cause of the disaster so they can be important for us. Therefore to use them effecticely we can add then to our text column and this will help us create a corpus that has all the words as well as keywords.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now I'll replace the keywords column in train with the preprocessed_keys.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_keys'] = processed_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(labels= 'keyword',inplace = True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id location                                               text  target  \\\n",
       "0   1      NaN  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1   4      NaN             Forest fire near La Ronge Sask. Canada       1   \n",
       "2   5      NaN  All residents asked to 'shelter in place' are ...       1   \n",
       "3   6      NaN  13,000 people receive #wildfires evacuation or...       1   \n",
       "4   7      NaN  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "  processed_keys  \n",
       "0            nan  \n",
       "1            nan  \n",
       "2            nan  \n",
       "3            nan  \n",
       "4            nan  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([15,30,5115],inplace = True)#dropping these rows as text values in these data point turn into null on preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and modeling only on keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707,) (1903,)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN TEST SPLIT OF THE KEYWORD COLUMN IN 75:25 RATIO.\n",
    "xktrain, xktest, yktrain, yktest = train_test_split(train['processed_keys'],train['target'], test_size=0.25)\n",
    "print(xktrain.shape, xktest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 230) (1903, 230)\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS ON KEYWORDS\n",
    "bow_keys = CountVectorizer()\n",
    "bow_keys.fit(train['processed_keys'])\n",
    "xktrain_bow = bow_keys.transform(xktrain)\n",
    "xktest_bow = bow_keys.transform(xktest)\n",
    "print(xktrain_bow.shape,xktest_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 230) (1903, 230)\n"
     ]
    }
   ],
   "source": [
    "# CREATING DATAFRAMES FOR BAG OF WORD VECTORS SO AS TO USE THEM IN FUTURE WITH \"TEXT\" CORPUS.\n",
    "dfk_train_bow = pd.DataFrame(xktrain_bow.todense(),columns= bow_keys.get_feature_names())\n",
    "dfk_test_bow = pd.DataFrame(xktest_bow.todense(),columns= bow_keys.get_feature_names())\n",
    "print(dfk_train_bow.shape,dfk_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 230) (1903, 230)\n"
     ]
    }
   ],
   "source": [
    "# TFIDF ON KEYWORDS\n",
    "tfidf_keys = TfidfVectorizer()\n",
    "tfidf_keys.fit(train['processed_keys'])\n",
    "xktrain_tfidf = tfidf_keys.transform(xktrain)\n",
    "xktest_tfidf= tfidf_keys.transform(xktest)\n",
    "print(xktrain_tfidf.shape,xktest_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 230) (1903, 230)\n"
     ]
    }
   ],
   "source": [
    "# CREATING DATAFRAMES FOR TFIDF VECTORS SO AS TO USE THEM IN FUTURE WITH \"TEXT\" CORPUS.\n",
    "dfk_train_tfidf = pd.DataFrame(xktrain_tfidf.todense(),columns= tfidf_keys.get_feature_names())\n",
    "dfk_test_tfidf = pd.DataFrame(xktest_tfidf.todense(),columns= tfidf_keys.get_feature_names())\n",
    "print(dfk_train_tfidf.shape,dfk_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have majorly used linear models here as they tend to perform better on presence of high dimensional data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      1088\n",
      "           1       0.70      0.65      0.67       815\n",
      "\n",
      "    accuracy                           0.73      1903\n",
      "   macro avg       0.73      0.72      0.72      1903\n",
      "weighted avg       0.73      0.73      0.73      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION ON BOW\n",
    "lr_key_bow = LogisticRegression(solver = 'liblinear')\n",
    "lr_key_bow.fit(xktrain_bow,yktrain)\n",
    "lr_key_bow_pred= lr_key_bow.predict(xktest_bow)\n",
    "print(classification_report(yktest,lr_key_bow_pred))\n",
    "# print(sns.heatmap(confusion_matrix(yktest,lr_key_bow_pred),cmap= 'coolwarm',annot = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      1088\n",
      "           1       0.70      0.65      0.67       815\n",
      "\n",
      "    accuracy                           0.73      1903\n",
      "   macro avg       0.73      0.72      0.72      1903\n",
      "weighted avg       0.73      0.73      0.73      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION ON TFIDF\n",
    "lr_key_tfidf = LogisticRegression(solver = 'liblinear')\n",
    "lr_key_tfidf.fit(xktrain_tfidf,yktrain)\n",
    "lr_key_tfidf_pred= lr_key_tfidf.predict(xktest_tfidf)\n",
    "print(classification_report(yktest,lr_key_tfidf_pred))\n",
    "# print(sns.heatmap(confusion_matrix(yktest,lr_key_tfidf_pred),cmap= 'coolwarm',annot = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      1088\n",
      "           1       0.71      0.64      0.67       815\n",
      "\n",
      "    accuracy                           0.73      1903\n",
      "   macro avg       0.73      0.72      0.72      1903\n",
      "weighted avg       0.73      0.73      0.73      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LINEAR SVM ON BOW                       --> RBF SVM PERFORMED VERY POORLY ON THIS DATA\n",
    "svm_key_bow = SVC(kernel = 'linear')\n",
    "svm_key_bow.fit(xktrain_bow,yktrain)\n",
    "svm_key_bow_pred= svm_key_bow.predict(xktest_bow)\n",
    "print(classification_report(yktest,svm_key_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      1088\n",
      "           1       0.70      0.65      0.67       815\n",
      "\n",
      "    accuracy                           0.73      1903\n",
      "   macro avg       0.73      0.72      0.72      1903\n",
      "weighted avg       0.73      0.73      0.73      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LINEAR SVM ON TFIDF                      --> RBF SVM PERFORMED VERY POORLY ON THIS DATA\n",
    "svm_key_tfidf= SVC(kernel = 'linear')\n",
    "svm_key_tfidf.fit(xktrain_tfidf,yktrain)\n",
    "svm_key_tfidf_pred= svm_key_tfidf.predict(xktest_tfidf)\n",
    "print(classification_report(yktest,svm_key_tfidf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As preprocessing the \"text\" column completely was time consuming every time I ran the notebook so I saved it in the form of a dataframe into a separate csv file named \"pp_tweets.csv\". I will mention the code snippet for preprocessing in commented form below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280    ---> found this really nice and short algo for preprocessing and so used it here.\n",
    "# def decontracted(phrase):\n",
    "#     # specific\n",
    "#     phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "#     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "#     # general\n",
    "#     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "#     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "#     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "#     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "#     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "#     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "#     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "#     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "#     return phrase\n",
    "\n",
    "# preprocessed_tweets = []\n",
    "# # tqdm is for printing the status bar\n",
    "# for sentance in tqdm(train['text'].values):\n",
    "#     sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "#     sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "#     sentance = decontracted(sentance)\n",
    "#     sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "#     sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "#     #preprocessed_tweets.append(' '.join(token.lower() for token in nltk.word_tokenize(sentance) if token.lower() not in stopwords.words('english')))\n",
    "#     sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords.words())\n",
    "#     preprocessed_tweets.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     processed_tweets  target\n",
      "15                NaN       0\n",
      "30                NaN       0\n",
      "5115              NaN       0\n",
      "                                    processed_tweets  target\n",
      "0       deeds reason earthquake may allah forgive us       1\n",
      "1                 forest fire near ronge sask canada       1\n",
      "2  residents asked ishelter place notified office...       1\n",
      "(7610, 2)\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('pp_tweets.csv')\n",
    "tweets.drop(labels = 'Unnamed: 0',axis = 1,inplace = True)\n",
    "print(tweets[tweets['processed_tweets'].isnull()==True])\n",
    "tweets.dropna(inplace = True)  # there were 3 null values in data after preprocessing so I had to remove them from the data.\n",
    "print(tweets.head(3))\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7610/7610 [00:01<00:00, 4382.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEMMING THE DATA AND ADDING IT TO DATAFRAME.\n",
    "new_list = []\n",
    "for text in tqdm(tweets['processed_tweets'].values):\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = []\n",
    "    for word in text:\n",
    "        word = stemmer.stem(word)\n",
    "        stemmed_text.append(word)\n",
    "    new_list.append(\" \".join(stemmed_text))    \n",
    "    \n",
    "tweets['stemmed_words']  = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweets</th>\n",
       "      <th>target</th>\n",
       "      <th>stemmed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>residents asked ishelter place notified office...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask ishelt place notifi offic evacu shel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    processed_tweets  target  \\\n",
       "0       deeds reason earthquake may allah forgive us       1   \n",
       "1                 forest fire near ronge sask canada       1   \n",
       "2  residents asked ishelter place notified office...       1   \n",
       "\n",
       "                                       stemmed_words  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1                  forest fire near rong sask canada  \n",
       "2  resid ask ishelt place notifi offic evacu shel...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707,) (1903,) (5707,) (1903,)\n"
     ]
    }
   ],
   "source": [
    "# SPLITTING THE DATA.\n",
    "xt_train, xt_test, yt_train, yt_test = train_test_split(tweets['stemmed_words'],tweets['target'] , test_size=0.25)\n",
    "print(xt_train.shape,xt_test.shape,yt_train.shape,yt_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 55374) (1903, 55374)\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS ON PROCESSED_TWEETS COLUMN.\n",
    "bow_text = CountVectorizer(ngram_range=(1,2))    # including uni-grams and bi-grams.\n",
    "bow_text.fit(tweets['stemmed_words'])\n",
    "xt_train_bow = bow_text.transform(xt_train)\n",
    "xt_test_bow = bow_text.transform(xt_test)\n",
    "print(xt_train_bow.shape, xt_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 55374) (1903, 55374)\n"
     ]
    }
   ],
   "source": [
    "# CREATING SEPARATE DATA FRAMES TO USE IN FUTURE MODELS\n",
    "dft_train_bow = pd.DataFrame(xt_train_bow.todense(),columns= bow_text.get_feature_names())\n",
    "dft_test_bow = pd.DataFrame(xt_test_bow.todense(),columns= bow_text.get_feature_names())\n",
    "print(dft_train_bow.shape,dft_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature names:  ['aa', 'aa ayyo', 'aa batteri', 'aa mgm', 'aaaa', 'aaaa ok', 'aaaaaaallll', 'aaaaaaallll even', 'aaaaaand', 'aaaaaand thunder', 'aaarrrgghhh', 'aac', 'aac org', 'aamir', 'aamir javaid', 'aannnnd', 'aannnnd reddit', 'aar', 'aar ambul', 'aaronthefm', 'aaronthefm guy', 'aashiqui', 'aashiqui actress', 'ab', 'ab resin', 'aba', 'aba woman', 'abandon', 'abandon aircraft', 'abandon cabin', 'abandon cocker', 'abandon deal', 'abandon lrt', 'abandon plan', 'abandon ship', 'abandon west', 'abandonedp', 'abandonedp delet', 'abbandon', 'abbandon sink', 'abbott', 'abbott campbel', 'abbott lnp', 'abbott must', 'abbruchsimul', 'abbswinston', 'abbswinston zionist', 'abbyairshow', 'abbyairshow offici', 'abc', 'abc aircraft', 'abc cbs', 'abc news', 'abc onlin', 'abc onlinea', 'abc pleas', 'abc trauma', 'abcnew', 'abcnew nuclear', 'abcnew obama', 'abcnew uk', 'abcnorio', 'abcnorio garden', 'abe', 'abe delug', 'abe govern', 'abe pledg', 'aberdeen', 'aberdeen tomorrow', 'aberdeenfanpag', 'aberdeenfanpag good', 'aberdeenfc', 'aberdeenfc aberdeenfanpag', 'aberystwyth', 'aberystwyth shrewsburi', 'abha', 'abha fatal', 'abil', 'abil annihil', 'abil massacr', 'abil offer', 'abil stand', 'abject', 'abject desol', 'abl', 'abl anyth', 'abl evacu', 'abl get', 'abl go', 'abl send', 'abl stay', 'abl support', 'abl surviv', 'abl target', 'abl touch', 'ablaz', 'ablaz aba', 'ablaz aliv', 'ablaz california', 'ablaz everi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Some feature names: \",bow_text.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 55374) (1903, 55374)\n"
     ]
    }
   ],
   "source": [
    "# TFIDF ON STEMMED DOCUMENTS\n",
    "tfidf_text = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_text.fit(tweets['stemmed_words'])\n",
    "xt_train_tfidf = tfidf_text.transform(xt_train)\n",
    "xt_test_tfidf = tfidf_text.transform(xt_test)\n",
    "print(xt_train_tfidf.shape,xt_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 55374) (1903, 55374)\n"
     ]
    }
   ],
   "source": [
    "# CREATING SEPARATE DATA FRAMES TO USE IN FUTURE MODELS\n",
    "dft_train_tfidf = pd.DataFrame(xt_train_tfidf.todense(),columns= tfidf_text.get_feature_names())\n",
    "dft_test_tfidf = pd.DataFrame(xt_test_tfidf.todense(),columns= tfidf_text.get_feature_names())\n",
    "print(dft_train_tfidf.shape,dft_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and modeling on Vectors obtained from preprocessed 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82      1033\n",
      "           1       0.83      0.67      0.74       870\n",
      "\n",
      "    accuracy                           0.79      1903\n",
      "   macro avg       0.80      0.78      0.78      1903\n",
      "weighted avg       0.79      0.79      0.78      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION ON BOW\n",
    "lrt_bow = LogisticRegression(solver = 'liblinear')\n",
    "lrt_bow.fit(xt_train_bow,yt_train)\n",
    "lrt_bow_pred = lrt_bow.predict(xt_test_bow)\n",
    "print(classification_report(yt_test,lrt_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71      1033\n",
      "           1       0.95      0.02      0.04       870\n",
      "\n",
      "    accuracy                           0.55      1903\n",
      "   macro avg       0.75      0.51      0.38      1903\n",
      "weighted avg       0.73      0.55      0.40      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION ON TFIDF\n",
    "lrt_tfidf = LogisticRegression(solver = 'liblinear')\n",
    "lrt_tfidf.fit(xt_train_tfidf,yt_train)\n",
    "lrt_tfidf_pred = lrt_bow.predict(xt_test_tfidf)\n",
    "print(classification_report(yt_test,lrt_tfidf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that Logistic regression is working significantly better on BOW than TFIDF vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81      1033\n",
      "           1       0.81      0.67      0.73       870\n",
      "\n",
      "    accuracy                           0.78      1903\n",
      "   macro avg       0.78      0.77      0.77      1903\n",
      "weighted avg       0.78      0.78      0.77      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LINEAR SVC ON BOW\n",
    "svmt_bow = SVC(kernel = 'linear')\n",
    "svmt_bow.fit(xt_train_bow,yt_train)\n",
    "svmt_bow_pred = svmt_bow.predict(xt_test_bow)\n",
    "print(classification_report(yt_test,svmt_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70      1033\n",
      "           1       1.00      0.00      0.00       870\n",
      "\n",
      "    accuracy                           0.54      1903\n",
      "   macro avg       0.77      0.50      0.35      1903\n",
      "weighted avg       0.75      0.54      0.38      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LINEAR SVC ON TFIDF\n",
    "svmt_tfidf = SVC(kernel = 'linear')\n",
    "svmt_tfidf.fit(xt_train_tfidf,yt_train)\n",
    "svmt_tfidf_pred = svmt_bow.predict(xt_test_tfidf)\n",
    "print(classification_report(yt_test,svmt_tfidf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that Linear SVM is also working significantly better on BOW than TFIDF vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.52      0.62      1033\n",
      "           1       0.59      0.80      0.68       870\n",
      "\n",
      "    accuracy                           0.65      1903\n",
      "   macro avg       0.67      0.66      0.65      1903\n",
      "weighted avg       0.68      0.65      0.64      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GAUSSIAN NAIVE BAYES ON BOW.\n",
    "nb_bow = GaussianNB()\n",
    "nb_bow.fit(xt_train_bow.toarray(),yt_train)\n",
    "nb_bow_pred = nb_bow.predict(xt_test_bow.toarray())\n",
    "print(classification_report(yt_test,nb_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.55      0.63      1033\n",
      "           1       0.59      0.77      0.67       870\n",
      "\n",
      "    accuracy                           0.65      1903\n",
      "   macro avg       0.66      0.66      0.65      1903\n",
      "weighted avg       0.67      0.65      0.65      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GAUSSIAN NAIVE BAYES ON TFIDF\n",
    "nb_tfidf = GaussianNB()\n",
    "nb_tfidf.fit(xt_train_tfidf.toarray(),yt_train)\n",
    "nb_tfidf_pred = nb_tfidf.predict(xt_test_tfidf.toarray())\n",
    "print(classification_report(yt_test,nb_tfidf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.52      0.62      1033\n",
      "           1       0.59      0.80      0.68       870\n",
      "\n",
      "    accuracy                           0.65      1903\n",
      "   macro avg       0.67      0.66      0.65      1903\n",
      "weighted avg       0.68      0.65      0.64      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST ON BOW\n",
    "rfct_bow = RandomForestClassifier(n_estimators=100)\n",
    "rfct_bow.fit(xt_train_bow.toarray(),yt_train)\n",
    "rfct_bow_pred = nb_bow.predict(xt_test_bow.toarray())\n",
    "print(classification_report(yt_test,rfct_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.93      0.81      1033\n",
      "           1       0.87      0.56      0.68       870\n",
      "\n",
      "    accuracy                           0.76      1903\n",
      "   macro avg       0.79      0.74      0.74      1903\n",
      "weighted avg       0.79      0.76      0.75      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST ON TFIDF\n",
    "rfct_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rfct_tfidf.fit(xt_train_tfidf.toarray(),yt_train)\n",
    "rfct_tfidf_pred = rfct_tfidf.predict(xt_test_tfidf.toarray())\n",
    "print(classification_report(yt_test,rfct_tfidf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL WAS TAKING TOO LONG TO EXECUTE SO COMMENTED IT OUT.\n",
    "# from datetime import datetime\n",
    "# start = datetime.now()\n",
    "# grid_rfct_tfidf = GridSearchCV(RandomForestClassifier(),param_grid={'n_estimators':[50,100,200,300]},cv = 3,iid = False,verbose = 2)\n",
    "# grid_rfct_tfidf.fit(xt_train_tfidf.toarray(),yt_train)\n",
    "# grid_rfct_tfidf_pred = grid_rfct_tfidf.predict(xt_test_tfidf)\n",
    "# print(grid_rfct_tfidf.best_params_)\n",
    "# print(classification_report(yt_test,grid_rfct_tfidf_pred))\n",
    "# print(\"Time taken to run thi cell: \",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 3238) (1903, 3238)\n"
     ]
    }
   ],
   "source": [
    "# CREATING BOW VECTORS BUT IGNORING FEATURES WITH LOW DOCUMENT FREQUENCY\n",
    "bow_text2 = CountVectorizer(ngram_range=(1,2),min_df= 0.0006)    # including uni-grams and bi-grams.\n",
    "bow_text2.fit(tweets['stemmed_words'])\n",
    "xt_train_bow2 = bow_text2.transform(xt_train)\n",
    "xt_test_bow2 = bow_text2.transform(xt_test)\n",
    "print(xt_train_bow2.shape, xt_test_bow2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 3238) (1903, 3238)\n"
     ]
    }
   ],
   "source": [
    "# CREATING TFIDF VECTORS BUT IGNORING FEATURES WITH LOW DOCUMENT FREQUENCY\n",
    "tfidf_text2 = TfidfVectorizer(ngram_range=(1,2),min_df=0.0006)\n",
    "tfidf_text2.fit(tweets['stemmed_words'])\n",
    "xt_train_tfidf2 = tfidf_text2.transform(xt_train)\n",
    "xt_test_tfidf2 = tfidf_text2.transform(xt_test)\n",
    "print(xt_train_tfidf2.shape,xt_test_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82      1033\n",
      "           1       0.82      0.70      0.75       870\n",
      "\n",
      "    accuracy                           0.79      1903\n",
      "   macro avg       0.79      0.78      0.78      1903\n",
      "weighted avg       0.79      0.79      0.79      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOGISTIC REGRESSION WITH BOW VECTORS WITH LESS NUMBER OF FEATURES.\n",
    "lrt_bow2 = LogisticRegression(solver = 'liblinear')\n",
    "lrt_bow2.fit(xt_train_bow2,yt_train)\n",
    "lrt_bow2_pred = lrt_bow2.predict(xt_test_bow2)\n",
    "print(classification_report(yt_test,lrt_bow2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82      1033\n",
      "           1       0.84      0.66      0.74       870\n",
      "\n",
      "    accuracy                           0.79      1903\n",
      "   macro avg       0.80      0.78      0.78      1903\n",
      "weighted avg       0.80      0.79      0.79      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOGISTIC REGRESSION WITH TFIDF VECTORS WITH LESS NUMBER OF FEATURES.\n",
    "lrt_tfidf2 = LogisticRegression(solver = 'liblinear')\n",
    "lrt_tfidf2.fit(xt_train_tfidf2,yt_train)\n",
    "lrt_tfidf2_pred = lrt_tfidf2.predict(xt_test_tfidf2)\n",
    "print(classification_report(yt_test,lrt_tfidf2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this we can see that by using only 3238 features we have performance comaparable to our normal logistic regression model using more than 55k features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 3238) (1903, 3238)\n"
     ]
    }
   ],
   "source": [
    "# CREATING DATAFRAMES OF BOW VECTORS WITH LESS FEATURES.\n",
    "dft_train_bow2 = pd.DataFrame(xt_train_bow2.todense(),columns= bow_text2.get_feature_names())\n",
    "dft_test_bow2 = pd.DataFrame(xt_test_bow2.todense(),columns= bow_text2.get_feature_names())\n",
    "print(dft_train_bow2.shape,dft_test_bow2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 3238) (1903, 3238)\n"
     ]
    }
   ],
   "source": [
    "# CREATING DATAFRAMES OF TFIDF VECTORS WITH LESS FEATURES.\n",
    "dft_train_tfidf2 = pd.DataFrame(xt_train_tfidf2.todense(),columns= tfidf_text2.get_feature_names())\n",
    "dft_test_tfidf2 = pd.DataFrame(xt_test_tfidf2.todense(),columns= tfidf_text2.get_feature_names())\n",
    "print(dft_train_tfidf2.shape,dft_test_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's concatenate the text and keyword datasets and see if the models performance improves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING FINAL DATAFRAMES BY CONCATENATING THE PREVIOUS DATAFRAMES.\n",
    "train_bow_large = pd.concat([dfk_train_bow,dft_train_bow],axis = 1)\n",
    "test_bow_large = pd.concat([dfk_test_bow,dft_test_bow],axis = 1)\n",
    "train_bow_small = pd.concat([dfk_train_bow,dft_train_bow2],axis = 1)\n",
    "test_bow_small = pd.concat([dfk_test_bow,dft_test_bow2],axis = 1)\n",
    "train_tfidf_large = pd.concat([dfk_train_tfidf,dft_train_tfidf],axis = 1)\n",
    "test_tfidf_large = pd.concat([dfk_test_tfidf,dft_test_tfidf],axis = 1)\n",
    "train_tfidf_small = pd.concat([dfk_train_tfidf,dft_train_tfidf2],axis = 1)\n",
    "test_tfidf_small = pd.concat([dfk_test_tfidf,dft_test_tfidf2],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 55604) (1903, 55604)\n",
      "(5707, 3468) (1903, 3468)\n",
      "(5707, 55604) (1903, 55604)\n",
      "(5707, 3468) (1903, 3468)\n"
     ]
    }
   ],
   "source": [
    "# CHECKING THE SHAPE OF THE DATAFRAMES.\n",
    "print(train_bow_large.shape,test_bow_large.shape)\n",
    "print(train_bow_small.shape,test_bow_small.shape)\n",
    "print(train_tfidf_large.shape,test_tfidf_large.shape)\n",
    "print(train_tfidf_small.shape,test_tfidf_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.81      1033\n",
      "           1       0.81      0.66      0.73       870\n",
      "\n",
      "    accuracy                           0.78      1903\n",
      "   macro avg       0.78      0.77      0.77      1903\n",
      "weighted avg       0.78      0.78      0.77      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING lOGISTIC REGRESSION WITH \"TEXT+KEYWORD\" BOW VECTORS WITH ALL FEATURES.\n",
    "lr_large_bow = LogisticRegression(solver= 'liblinear')\n",
    "lr_large_bow.fit(train_bow_large,yt_train)\n",
    "lr_large_bow_pred= lr_large_bow.predict(test_bow_large)\n",
    "print(classification_report(yt_test,lr_large_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80      1033\n",
      "           1       0.79      0.67      0.72       870\n",
      "\n",
      "    accuracy                           0.77      1903\n",
      "   macro avg       0.77      0.76      0.76      1903\n",
      "weighted avg       0.77      0.77      0.76      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING lOGISTIC REGRESSION WITH \"TEXT+KEYWORD\" BOW VECTORS WITH ALL FEATURES WITH DIFF. HYPERPARAMETERS.\n",
    "lr_large_bow1 = LogisticRegression(solver= 'liblinear',penalty = 'l1', C = 1.8)#even on hyperparameter tuning the max. F1 score \n",
    "lr_large_bow1.fit(train_bow_large,yt_train)                                  # was same as above.\n",
    "lr_large_bow_pred1= lr_large_bow1.predict(test_bow_large)\n",
    "print(classification_report(yt_test,lr_large_bow_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80      1033\n",
      "           1       0.84      0.56      0.67       870\n",
      "\n",
      "    accuracy                           0.75      1903\n",
      "   macro avg       0.78      0.74      0.73      1903\n",
      "weighted avg       0.77      0.75      0.74      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING lOGISTIC REGRESSION WITH \"TEXT+KEYWORD\" TFIDF VECTORS WITH ALL FEATURES.\n",
    "lr_large_tfidf = LogisticRegression(solver= 'liblinear')\n",
    "lr_large_tfidf.fit(train_tfidf_large,yt_train)\n",
    "lr_large_tfidf_pred= lr_large_tfidf.predict(test_tfidf_large)\n",
    "print(classification_report(yt_test,lr_large_tfidf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.81      1033\n",
      "           1       0.80      0.69      0.74       870\n",
      "\n",
      "    accuracy                           0.78      1903\n",
      "   macro avg       0.78      0.77      0.77      1903\n",
      "weighted avg       0.78      0.78      0.78      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING lOGISTIC REGRESSION WITH \"TEXT+KEYWORD\" BOW VECTORS WITH LESS FEATURES.\n",
    "lr_small_bow = LogisticRegression(solver= 'liblinear')\n",
    "lr_small_bow.fit(train_bow_small,yt_train)\n",
    "lr_small_bow_pred= lr_small_bow.predict(test_bow_small)\n",
    "print(classification_report(yt_test,lr_small_bow_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81      1033\n",
      "           1       0.81      0.67      0.73       870\n",
      "\n",
      "    accuracy                           0.78      1903\n",
      "   macro avg       0.78      0.77      0.77      1903\n",
      "weighted avg       0.78      0.78      0.77      1903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING lOGISTIC REGRESSION WITH \"TEXT+KEYWORD\" TFIDF VECTORS WITH LESS FEATURES.\n",
    "lr_small_tfidf = LogisticRegression(solver= 'liblinear')\n",
    "lr_small_tfidf.fit(train_tfidf_small,yt_train)\n",
    "lr_small_tfidf_pred= lr_small_tfidf.predict(test_tfidf_small)\n",
    "print(classification_report(yt_test,lr_small_tfidf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this we can see that, for F1 score, our Logistic Regression model trained on only the text column with all features and  Logistic Regression trained on less text features have the same performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMING TEST DATA AND SAVING FINAL SUBMISSION FILE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3263/3263 [09:07<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "preprocessed_tweets = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(test['text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    #preprocessed_tweets.append(' '.join(token.lower() for token in nltk.word_tokenize(sentance) if token.lower() not in stopwords.words('english')))\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords.words())\n",
    "    preprocessed_tweets.append(sentance.strip())\n",
    "    \n",
    "test['processed_tweets'] = preprocessed_tweets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3263/3263 [00:00<00:00, 4430.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEMMING THE DATA AND ADDING IT TO DATAFRAME.\n",
    "new_list = []\n",
    "for text in tqdm(test['processed_tweets'].values):\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = []\n",
    "    for word in text:\n",
    "        word = stemmer.stem(word)\n",
    "        stemmed_text.append(word)\n",
    "    new_list.append(\" \".join(stemmed_text))    \n",
    "    \n",
    "test['stemmed_words']  = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_tweets</th>\n",
       "      <th>stemmed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>happen terribl car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills china taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                    processed_tweets  \\\n",
       "0                        happened terrible car crash   \n",
       "1  heard earthquake different cities stay safe ev...   \n",
       "2  forest fire spot pond geese fleeing across str...   \n",
       "3              apocalypse lighting spokane wildfires   \n",
       "4                typhoon soudelor kills china taiwan   \n",
       "\n",
       "                                       stemmed_words  \n",
       "0                           happen terribl car crash  \n",
       "1      heard earthquak differ citi stay safe everyon  \n",
       "2  forest fire spot pond gees flee across street ...  \n",
       "3                     apocalyps light spokan wildfir  \n",
       "4                 typhoon soudelor kill china taiwan  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW\n",
    "t_bow_vec = bow_text.transform(test['stemmed_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = lrt_bow.predict(t_bow_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_target'] = final_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_tweets</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>pred_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>happen terribl car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills china taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                    processed_tweets  \\\n",
       "0                        happened terrible car crash   \n",
       "1  heard earthquake different cities stay safe ev...   \n",
       "2  forest fire spot pond geese fleeing across str...   \n",
       "3              apocalypse lighting spokane wildfires   \n",
       "4                typhoon soudelor kills china taiwan   \n",
       "\n",
       "                                       stemmed_words  pred_target  \n",
       "0                           happen terribl car crash            1  \n",
       "1      heard earthquak differ citi stay safe everyon            1  \n",
       "2  forest fire spot pond gees flee across street ...            1  \n",
       "3                     apocalyps light spokan wildfir            1  \n",
       "4                 typhoon soudelor kill china taiwan            1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.concat([test['id'],test['pred_target']],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pred_target\n",
       "0   0            1\n",
       "1   2            1\n",
       "2   3            1\n",
       "3   9            1\n",
       "4  11            1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = sub['pred_target']\n",
    "sub.drop(\"pred_target\",axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"first_submission.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
